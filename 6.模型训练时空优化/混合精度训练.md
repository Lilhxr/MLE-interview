# 混合精度训练

通常我们训练神经网络的时候，默认的数据类型为**单精度浮点数FP32**。但是，为了加快训练**时间**、减少网络训练时候所占用的**内存**，业界提出越来越多的混合精度训练的方法。这里的混合精度训练是指：**在训练的过程中，同时使用单精度（FP32）和半精度（FP16来表示网络模型权重和其他参数）**



## 1. 计算机中的浮点数表示

根据IEEE二进制浮点数算术标准，浮点数据类型分为双精度（FP64）、单精度（FP32）、半精度（FP16）三种。浮点数的表示有三个段：符号位(sign), 阶码(exp), 尾码(frac). 

![fp16-vs-fp32](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/tutorials/experts/source_zh_cn/others/images/fp16_vs_fp32.png)



以FP16为例子，第一位符号位sign表示正负符号，接着5位表示阶码E，最后10位表示尾码fraction。公式为：

​                                                               $x=(−1)^S×2^{E−15}×(1+fraction/1024)$

FP32：

​                                                                             $x=(−1)^S×2^{E−127}×(1.M)$

FP64：

​                                                                             $x=(−1)^S×2^{E−1023}×(1.M)$

FP16可以表示的最大值为` 0 11110 1111111111`，其中 $E = 30$，$frac = 2^{10}-1$, 对应的值为 $2^{30-15} \times (1+\frac{2^{10}-1}{1024}) = 65504 $.

FP16可以表示的最小值为 `0 00001 0000000000`，其中$E = 1, frac = 0$, 对应的值为 $2^{-14} \approx 6.104 \times 10^{-5}$

因此FP16的最大取值范围是 +-$[6.104 \times 10^{-5}, 66504]$，大于/小于这个数值的数字会被直接置0。



## 2. FP16计算的好处

使用FP16训练神经网络，比使用FP32带来的优点有：

- 减少内存占用：FP16的位宽是FP32的一半，因此权重等参数所**占用的内存也是原来的一半**，节省下来的内存可以放**更大的网络模型**，还可以用来存储**更大的数据集**。
- 加快**通讯**效率：针对**分布式训练**，特别是在大模型训练的过程中，通讯的开销制约了网络模型训练的整体性能，通讯的位宽少了意味着可以提升通讯性能，减少等待时间，加快数据的流通。
- **计算效率更高**：在特殊的AI加速芯片如昇腾系列的Ascend 910和310系列，或者NVIDIA VOLTA架构的GPU上，使用FP16的执行运算性能比FP32更加快。

**注意：小batch场景下混合精度并不能带来速度提升，甚至会更慢。因为小batch下的计算已经很快了，速度瓶颈在IO（在GPU和GPU间传送数据）。而混合精度需要进行FP16与FP32的转换，会消耗更多时间。**



## 3. FP16计算的问题

但是使用FP16同样会带来一些问题，其中最重要的是**精度溢出**和**舍入误差**。

- 数据溢出：数据溢出比较好理解，`FP16`的有效数据表示范围为 $[6.10×10^{−5},65504]$，`FP32`的有效数据表示范围为 $[1.4×10^{−45},1.7×10^{38}]$。可见FP16相比FP32的有效范围要窄很多，使用FP16替换FP32会出现**上溢（Overflow）和下溢（Underflow）**的情况。而在深度学习中，需要计算网络模型中权重的梯度（一阶导数），梯度会比权重值更加小，往往**容易出现下溢情况**，导致都变成0.
- 舍入误差：如0.00006666666在FP32中能正常表示，转换到FP16后会表示成为0.000067，会强制舍入。

## 4. 混合精度计算流程

![mix precision](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/tutorials/experts/source_zh_cn/others/images/mix_precision_fp16.png)

1. 参数以FP32存储。用FP32保存权重主要是为了避免溢出，因为FP16无法表示$2^{-14}$以下的值，一是梯度的更新值太小，FP16直接变为了0；二是FP16表示权重的话，和梯度的计算结果也有可能变成0。实验表明，用FP16保存权重会造成80%的精度损失。
2. 正向计算过程中，遇到FP16算子，需要把算子输入和参数从FP32 cast成FP16进行计算；
3. 将Loss层设置为FP32进行计算；
4. 反向计算过程中，首先乘以Loss Scale值，避免反向梯度过小而产生下溢；
5. FP16参数参与梯度计算，其结果将被cast回FP32；
6. 除以Loss scale值，还原被放大的梯度；
7. 判断梯度是否存在溢出，如果溢出则跳过更新，否则优化器以FP32对原始参数进行更新。

### 4.1 自动混合精度

使用自动混合精度，需要调用`Model`接口，将待训练网络和优化器作为输入传进去，该接口会把网络模型的的算子转换成FP16算子。

> 除`BatchNorm`算子和Loss涉及到的算子外因为精度问题，仍然使用FP32执行运算。

使用`Model`接口具体的实现步骤为：

1. 引入MindSpore的模型训练接口`Model`；
2. 定义网络：该步骤和正常的网络定义相同(无需新增任何配置)；
3. 创建数据集：该步骤可参考[数据处理](https://www.mindspore.cn/tutorials/zh-CN/r1.8/advanced/dataset.html)；
4. 使用`Model`接口封装网络模型、优化器和损失函数，设置`amp_level`参数，详情参考[MindSpore API](https://www.mindspore.cn/docs/zh-CN/r1.8/api_python/mindspore/mindspore.Model.html#mindspore.Model)。该步骤MindSpore会自动选择合适的算子自动进行FP32到FP16的类型转换。

下面是基础的代码样例，首先导入必须的库和声明，并定义LeNet5网络模型。

```python
import numpy as np
import mindspore.nn as nn
from mindspore.nn import Accuracy
import mindspore as ms
from mindspore.common.initializer import Normal
from mindspore import dataset as ds

ms.set_context(mode=ms.GRAPH_MODE)
ms.set_context(device_target="GPU")

class LeNet5(nn.Cell):
    """
    Lenet network

    Args:
        num_class (int): Number of classes. Default: 10.
        num_channel (int): Number of channels. Default: 1.

    Returns:
        Tensor, output tensor
    """

    def __init__(self, num_class=10, num_channel=1):
        super(LeNet5, self).__init__()
        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')
        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')
        self.fc1 = nn.Dense(16 * 5 * 5, 120, weight_init=Normal(0.02))
        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))
        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.02))
        self.relu = nn.ReLU()
        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)
        self.flatten = nn.Flatten()

    def construct(self, x):
        x = self.max_pool2d(self.relu(self.conv1(x)))
        x = self.max_pool2d(self.relu(self.conv2(x)))
        x = self.flatten(x)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x
```

接着创建一个虚拟的随机数据集，用于样例模型的数据输入。

```
# create dataset
def get_data(num, img_size=(1, 32, 32), num_classes=10, is_onehot=True):
    for _ in range(num):
        img = np.random.randn(*img_size)
        target = np.random.randint(0, num_classes)
        target_ret = np.array([target]).astype(np.float32)
        if is_onehot:
            target_onehot = np.zeros(shape=(num_classes,))
            target_onehot[target] = 1
            target_ret = target_onehot.astype(np.float32)
        yield img.astype(np.float32), target_ret

def create_dataset(num_data=1024, batch_size=32, repeat_size=1):
    input_data = ds.GeneratorDataset(list(get_data(num_data)), column_names=['data', 'label'])
    input_data = input_data.batch(batch_size, drop_remainder=True)
    input_data = input_data.repeat(repeat_size)
    return input_data
```

设置`amp_level`参数，使用`Model`接口封装网络模型、优化器和损失函数。

```
ds_train = create_dataset()

# Initialize network
network = LeNet5(10)

# Define Loss and Optimizer
net_loss = nn.SoftmaxCrossEntropyWithLogits(reduction="mean")
net_opt = nn.Momentum(network.trainable_params(), learning_rate=0.01, momentum=0.9)
# Set amp level
model = ms.Model(network, net_loss, net_opt, metrics={"Accuracy": Accuracy()}, amp_level="O3")

# Run training
model.train(epoch=10, train_dataset=ds_train)
TotalTime = 1.29408, [19]
[parse]: 0.449277
[symbol_resolve]: 0.524724, [1]
    [Cycle 1]: 0.524189, [1]
        [resolve]: 0.524178
[combine_like_graphs]: 0.00118984
[inference_opt_prepare]: 0.000437367
[elininate_unused_parameter]: 0.000120089
[abstract_specialize]: 0.0697742
[auto_monad]: 0.00147299
[inline]: 8.08202e-06
[py_pre_ad]: 8.5216e-07
[pipeline_split]: 6.4522e-06
[optimize]: 0.209156, [14]
    [simplify_data_structures]: 0.000270152
    [opt_a]: 0.204985, [3]
        [Cycle 1]: 0.179181, [25]
            [expand_dump_flag]: 1.03367e-05
            [switch_simplify]: 0.000675488
            [a_1]: 0.00937402
            [recompute_prepare]: 8.37147e-05
            [updatestate_depend_eliminate]: 0.000645219
            [updatestate_assign_eliminate]: 6.78673e-05
            [updatestate_loads_eliminate]: 0.000229622
            [parameter_eliminate]: 2.73902e-06
...
 0.03% :     0.000172s :      1: opt.transforms.opt_after_cconv
 0.01% :     0.000050s :      1: opt.transforms.opt_b
 0.05% :     0.000266s :      1: opt.transforms.opt_trans_graph
 0.00% :     0.000024s :      1: opt.transforms.stop_gradient_special_op
```

> - **amp_level** (str) - mindspore.build_train_network 的可选参数 level ， level 为混合精度等级，该参数支持[“O0”, “O2”, “O3”, “auto”]。默认值：”O0”。
>
>   - “O0”: 不变化。
>   - “O2”: 将网络精度转为float16，BatchNorm保持float32精度，使用动态调整损失缩放系数（loss scale）的策略。
>   - “O3”: 将网络精度（包括BatchNorm）转为float16，不使用损失缩放策略。
>   - auto: 为不同处理器设置专家推荐的混合精度等级，如在GPU上设为”O2”，在Ascend上设为”O3”。该设置方式可能在部分场景下不适用，建议用户根据具体的网络模型自定义设置 amp_level 。
>
>   在GPU上建议使用”O2”，在Ascend上建议使用”O3”。 通过 kwargs 设置 keep_batchnorm_fp32 ，可修改BatchNorm的精度策略， keep_batchnorm_fp32 必须为bool类型；通过 kwargs 设置 loss_scale_manager 可修改损失缩放策略，loss_scale_manager 必须为 `mindspore.LossScaleManager` 的子类， 关于 amp_level 详见 mindpore.build_train_network 。





### 4.2 手动混合精度

MindSpore目前还支持手动混合精度（一般不建议使用手动混合精度，除非自定义特殊网络和特性开发）。

假定在网络中只有一个Dense Layer使用FP16计算，其他Layer都用FP32计算。



```
ds_train = create_dataset()
network = LeNet5(10)
net_loss = nn.SoftmaxCrossEntropyWithLogits(reduction="mean")
net_opt = nn.Momentum(network.trainable_params(), learning_rate=0.01, momentum=0.9)
network.conv1.to_float(ms.float16) ##在定义网络后，在初始化网络模型的时候声明dense层使用FP16进行计算
model = ms.Model(network, net_loss, net_opt, metrics={"Accuracy": Accuracy()}, amp_level="O2")

model.train(epoch=2, train_dataset=ds_train)

```



## 5. 损失缩放原理

损失缩放（Loss Scale）技术主要是作用于混合精度训练的过程当中。

在混合精度训练的过程中，会使用FP16类型来替代FP32类型进行数据存储，从而达到减少内存和提高计算速度的效果。但是由于FP16类型要比FP32类型表示的**范围小很多**，所以当参数（或梯度）在训练过程中变得很小时，就会发生下溢的情况，导致较小的值都变成0。而损失缩放，正是为了解决FP16类型**数据下溢**问题而提出的。

如图所示，如果仅仅使用FP32训练，模型收敛得比较好，但是如果用了混合精度训练，会存在网络模型无法收敛的情况。原因是梯度的值太小，使用FP16表示会造成了数据下溢（Underflow）的问题，导致模型不收敛，如图中灰色的部分。

![loss-scale1](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/tutorials/experts/source_zh_cn/others/images/loss_scale1.png)

于是需要引入损失缩放（Loss Scale）技术。其主要思想是在计算损失值loss的时候，将loss**扩大一定的倍数**。由于**链式法则**的存在，**每一层的梯度也会顺便扩大**，然后在优化器更新权重时再**缩小相应的倍数**，从而避免了数据下溢的情况又**不影响计算结果**。

MindSpore中提供了两种Loss Scale的方式，分别是`FixedLossScaleManager`和`DynamicLossScaleManager`，需要和Model配合使用。在使用Model构建模型时，可配置混合精度策略`amp_level`和Loss Scale方式`loss_scale_manager`。

下面是在网络模型训练阶段某一层的梯度分布。可以看到，其中有68%的网络模型激活参数为0，另外有4%的精度在$[2^{−32},2^{−20}]$这个区间内，直接使用FP16对这里面的数据进行表示，会截断下溢的数据，所有的梯度值都会变为0。

![loss-scale2](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/tutorials/experts/source_zh_cn/others/images/loss_scale2.png)

为了解决梯度过小数据下溢的问题，我们可以对前向计算出来的Loss值进行**放大**操作，也就是把FP32的参数乘以某一个因子系数后，把可能溢出的小数位数据往前移，**平移到FP16能表示的数据范围内**。根据链式求导法则，**放大Loss后会作用在反向传播的每一层梯度，这样比在每一层梯度上进行放大更加高效**。

![loss-scale3](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/tutorials/experts/source_zh_cn/others/images/loss_scale3.png)

(FP16 denorms 值得是subnormal浮点数表示)

损失放大是需要结合混合精度实现的，其主要的主要思路是：

- **Scale up阶段**：网络模型前向计算后，在反向传播前，将得到的Loss增大$2^K$倍。
- **Scale down阶段**：反向传播后，将权重**梯度**缩$2^K$倍，**恢复FP32值进行存储**。



**动态损失缩放（Dynamic Loss Scale）**：上面提到的损失缩放都是使用一个**默认值**对损失值进行缩放，为了充分利用FP16的动态范围，可以更好地缓解舍入误差，**尽量使用比较大的放大倍数**。总结动态损失缩放算法，就是每当梯度上溢的时候减少损失缩放规模，并且间歇性地尝试增加损失规模，从而实现在不引起溢出的情况下使用最**高损失缩放因子**，更好地恢复精度。

动态损失缩放的算法如下：

1. 动态损失缩放的算法会从比较高的缩放因子开始（如$2^{24}$），然后开始进行训练，并在迭代中检查数是否会溢出（Infs/Nans）；
2. 如果没有梯度溢出，则不调整缩放因子，继续进行迭代；如果检测到梯度溢出，则缩放因子会**减半**，重新确认梯度更新情况，直到参数不出现在溢出的范围内；
3. 在训练的后期，loss已经趋近收敛稳定，梯度更新的幅度往往更小了，这个时候可以允许**更高的损失缩放因子**来再次防止数据下溢。
4. 因此，动态损失缩放算法会尝试在每N（N=2000）次迭代将损失缩放增加倍数，然后执行步骤2检查是否溢出。



下面将会分别介绍MindSpore中，使用损失缩放算法的主要两个API [FixedLossScaleManager](https://www.mindspore.cn/docs/zh-CN/r1.8/api_python/amp/mindspore.amp.FixedLossScaleManager.html)和[DynamicLossScaleManager](https://www.mindspore.cn/docs/zh-CN/r1.8/api_python/amp/mindspore.amp.DynamicLossScaleManager.html)。

###### FixedLossScaleManager

`FixedLossScaleManager`在进行缩放的时候，缩放的scale是不会变的，一直都是loss_scale。用户不指定则取默认值。

`FixedLossScaleManager`的另一个参数是`drop_overflow_update`，用来控制发生溢出时是否不更新参数。默认值为True，即：发生溢出时不更新参数。使用`FixedLossScaleManager`时，如果`drop_overflow_update`为False，那么**优化器**需设置`loss_scale`的值，且`loss_scale`的值要与`FixedLossScaleManager`的相同。这是由于采用此方式进行配置时，梯度与`loss_scale`系数之间的除法运算在优化器中进行。优化器设置与`FixedLossScaleManager`相同的`loss_scale`，训练结果才是正确的。

`FixedLossScaleManager`具体用法如下：

1. 真正使用Loss Scale的API接口，作用于优化器和模型中。

```python
#1) 如果溢出，则不更新参数：
loss_scale_manager = ms.FixedLossScaleManager()
net_opt = nn.Momentum(network.trainable_params(), learning_rate=0.01, momentum=0.9)
model = ms.Model(network, net_loss, net_opt, metrics={"Accuracy": Accuracy()}, amp_level="O0", loss_scale_manager=loss_scale_manager)

#2) 即使溢出，也更新参数
loss_scale_manager = ms.FixedLossScaleManager(1024.0, False)
net_opt = nn.Momentum(network.trainable_params(), learning_rate=0.01, momentum=0.9, loss_scale=loss_scale) ##优化器也需要设置loss_scale值
model = ms.Model(network, net_loss, net_opt, metrics={"Accuracy": Accuracy()}, amp_level="O0", loss_scale_manager=loss_scale_manager)

model.train(epoch=10, train_dataset=ds_train, callbacks=[ms.LossMonitor()])
```



###### DynamicLossScaleManager

`DynamicLossScaleManager`在训练过程中可以动态改变scale的大小，在没有发生溢出的情况下，要尽可能保持较大的scale。

`DynamicLossScaleManager`会首先将scale设置为一个初始值，该值由入参init_loss_scale控制。

在训练过程中，如果不发生溢出，在scale_window个step后，会尝试扩大scale的值，如果发生了溢出，则跳过参数更新，并缩小scale的值，入参scale_factor是控制扩大或缩小的步数，scale_window控制没有发生溢出时，最大的连续更新步数。

具体用法如下：

```python
# Define Loss Scale, optimizer and model
scale_factor = 4
scale_window = 3000
loss_scale_manager = ms.DynamicLossScaleManager(scale_factor, scale_window)
net_opt = nn.Momentum(network.trainable_params(), learning_rate=0.01, momentum=0.9)
model = ms.Model(network, net_loss, net_opt, metrics={"Accuracy": Accuracy()}, amp_level="O0", loss_scale_manager=loss_scale_manager)
```