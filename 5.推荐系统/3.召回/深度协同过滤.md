# 深度协同过滤

##### 1. 协同过滤的思想



基于矩阵分解(matrix factorization)的协同过滤把userID和itemID打成embedding，让其试图恢复原先的user-item矩阵，然后在未见的user-item交互中可以预测该处user-item矩阵的值（用点积来计算），其本质就是矩阵补全问题。



##### 2. 图网络做协同过滤

##### 2.1 Neural Graph Collaborative Filtering (SIGIR 2019)

基于矩阵分解的协同过滤得到user/item 的 ID embedding，不免会遇到**数据稀疏**问题。同时，矩阵分解的协同过滤**只能把握一阶的user-item关系，不能得到高阶的关系**。何谓“高阶的连接”？下图中i4->u2->i2->u1就是一个高阶的关系。user2喜欢item2，同时user1也喜欢item2，这就把user1和user2显式的连接起来了。

![img](https://pic1.zhimg.com/80/v2-feffda0f5a3525b285c700cfd6f66d0c_1440w.png)

为了得到**显式的高阶的连接性(high-order connectivity)**, 我们把user-item矩阵用一个图来表示。让图**聚合**k次邻居节点的信息，就能够把握住k阶的连接性。如上图中，聚合两次就可以显式把握住u1->i2->u2的联系，聚合三次就可以显式把握住u1->i2->u2->i4的联系。

具体的做法是，首先将经过MF得到的user/item ID embedding作为每个节点的**初始化**embedding；然后，在图上对邻居（user的邻居是item，item的邻居是user）进行**聚合**以把握住高阶的连接关系，以此来对MF得到的初始embedding做更精细的调整(refine). 下面，我们来详细介绍图中是如何聚合的。

1）信息传递

对于相连的user-item对(u,i), i传到u的信息计算公式：

![img](https://pic3.zhimg.com/80/v2-f89133cc1f642cd880c95fd438cf05c9_1440w.jpeg)

和普通的GCN邻居聚合方法不同的是，这里还增加了item embedding和邻居user embedding的哈达玛积，引入了交互信息。那么，第一次聚合之后，user的表示为：

![img](https://pic2.zhimg.com/80/v2-b727f0391bdbda5c056aeed942276f75_1440w.png)

即为user本身，传到user的信息+所有邻居item传到user的信息。经过L次如此的聚合，我们就可以得到融合了L阶关系的user/item embedding。这L个不同阶的embedding我们可以将他们进行拼接/加权/过LSTM等方式融合，得到最终的user/item embedding。这个embedding比起简单的用矩阵分解方式得到的embedding就融合了不同阶的user-item交互信息，因此包含的信息更为丰富了。

这样，我们就得到了user/item embedding，二者求点积即为预测的得分（相似度）。损失函数选用pair-wise的Bayesian personalized ranking (BPR) loss，即：

![img](https://pic3.zhimg.com/80/v2-3ecec4de701beadbb5ca95590b774048_1440w.png)

其中，i为正样本，j为负样本。

